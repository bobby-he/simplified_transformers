device_train_batch_size: 32
device_eval_batch_size: 32
gradient_accumulation_steps: 4
num_train_epochs: 1
weight_decay: 0.1
warmup_ratio: 0.05
lr_scheduler_type: 'polynomial'
learning_rate: 1e-3
adam_epsilon: 1e-8
optim: adamw_hf
max_grad_norm: 1